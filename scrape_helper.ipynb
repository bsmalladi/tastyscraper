{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "senior-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "injured-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasty_scrape(search, rec_type=\"recipe\"):\n",
    "    '''\n",
    "    scrapes tasty.co for recipes matching *search*\n",
    "    RETURNS DataFrame of:\n",
    "        'href': \n",
    "            rec_type==\"all\" ---> all recipe links on page\n",
    "            rec_type==\"recipe\" ---> all singular recipes on page\n",
    "            rec_type==\"compilation\" ---> all recipe compilations on page\n",
    "        'title':\n",
    "            title of link\n",
    "    '''\n",
    "    # selenium driver setup\n",
    "    print(\"retrieving webpage...\")\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    node_modules_bin = subprocess.run(\n",
    "        [\"npm\", \"bin\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        universal_newlines=True,\n",
    "        check=True\n",
    "    )\n",
    "    node_modules_bin_path = node_modules_bin.stdout.strip()\n",
    "    chromedriver_path = Path(node_modules_bin_path) / \"chromedriver\"\n",
    "\n",
    "    driver = selenium.webdriver.Chrome(\n",
    "        options=options,\n",
    "        executable_path=str(chromedriver_path),\n",
    "    )\n",
    "    driver.implicitly_wait(1)\n",
    "    url = \"https://tasty.co/search?q=\" + search\n",
    "    driver.get(url)\n",
    "\n",
    "    # click \"see more\" button repeatedly\n",
    "    print(\"clicking buttons...\")\n",
    "    bttndiv = driver.find_elements_by_xpath(\"//div[@class='show-more']\")\n",
    "    while (bttndiv):\n",
    "        bttn = bttndiv[0].find_elements_by_tag_name('button')[0]\n",
    "        bttn.click()\n",
    "        time.sleep(0.2)\n",
    "        bttndiv = driver.find_elements_by_xpath(\"//div[@class='show-more']\")\n",
    "\n",
    "    # compile all recipe data\n",
    "    print(\"getting all links...\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    all_links = soup.find_all('a', class_='feed-item')\n",
    "    data = {\n",
    "        'href': [],\n",
    "        'title': [],\n",
    "        'rectype': [],\n",
    "    }\n",
    "    for link in all_links:\n",
    "        ttl = link.find('div', class_='feed-item__title').text\n",
    "        href = link['href']\n",
    "        rtype = href.split('/')[1]\n",
    "        if rec_type == 'all' or rtype == rec_type:\n",
    "            url = \"https://tasty.co\" + href\n",
    "            data['href'].append(url)\n",
    "            data['title'].append(ttl)\n",
    "            data['rectype'].append(rtype)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "placed-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasty_recipe_scraper(url):\n",
    "    '''\n",
    "    INPUT: url of recipe to be scraped\n",
    "    OUTPUT: dictionary of recipe's info\n",
    "    '''\n",
    "    recipe_page = requests.get(url)\n",
    "    soup = BeautifulSoup(recipe_page.text, 'html.parser')\n",
    "    info = {\n",
    "        'title': '',\n",
    "        'link': url,\n",
    "        'score': None,\n",
    "        'total_time': None,\n",
    "        'prep': None,\n",
    "        'cook': None,\n",
    "        'num_ing': 0,\n",
    "        'num_steps': 0\n",
    "    }\n",
    "    title_element = soup.find('h1', class_='recipe-name')\n",
    "    if title_element:\n",
    "        info['title'] = title_element.text\n",
    "    scoretext = soup.find('h4', class_='tips-score-heading')\n",
    "    if scoretext:\n",
    "        info['score'] = int(scoretext.text.split('%')[0])  \n",
    "    times = soup.find_all('div', class_='recipe-time')\n",
    "    if times:\n",
    "        info['total_time'] = times[0].find('p').text\n",
    "        info['prep'] = times[1].find('p').text\n",
    "        info['cook'] = times[2].find('p').text\n",
    "    ingredients = soup.find_all('li', class_='ingredient')\n",
    "    info['num_ing'] = len(ingredients)\n",
    "    steps = soup.find('ol', class_='prep-steps').find_all('li')\n",
    "    info['num_steps'] = len(steps)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasty_compilation_scraper(url):\n",
    "    '''\n",
    "    INPUT: URL for a recipe compilation page\n",
    "    OUTPUT: DataFrame with recipe info \n",
    "            compatible with scrape_all_recipes.all_recipe_info\n",
    "    '''\n",
    "    all_recipe_info = pd.DataFrame({\n",
    "        'title': [],\n",
    "        'link': [],\n",
    "        'score': [],\n",
    "        'total_time': [],\n",
    "        'prep': [],\n",
    "        'cook': [],\n",
    "        'num_ing': [],\n",
    "        'num_steps': []\n",
    "    })\n",
    "    comp_page = requests.get(url)\n",
    "    soup = BeautifulSoup(comp_page.text, 'html.parser')\n",
    "    all_recipes = soup.find_all('a', class_='feed-item')\n",
    "    for recipe in all_recipes:\n",
    "        url = \"https://tasty.co\" + recipe['href'] \n",
    "        info = tasty_recipe_scraper(url)\n",
    "        all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "    return all_recipe_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_comps(compdf):\n",
    "    '''\n",
    "    TODO: scrape all compilations in compdf\n",
    "    scrapes each compilation page in DataFrame compdf\n",
    "    returns DataFrame with all recipe info\n",
    "    '''\n",
    "    all_recipe_info = pd.DataFrame({\n",
    "        'title': [],\n",
    "        'link': [],\n",
    "        'score': [],\n",
    "        'total_time': [],\n",
    "        'prep': [],\n",
    "        'cook': [],\n",
    "        'num_ing': [],\n",
    "        'num_steps': []\n",
    "    })\n",
    "    for index, row in compdf.iterrows():\n",
    "        info = tasty_compilation_scraper(row['href'])\n",
    "        all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "    return all_recipe_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "premier-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_types(alldf):\n",
    "    '''\n",
    "    TODO: scrape all types of recipes in alldf\n",
    "    scrapes each compilation or recipe page in DataFrame alldf\n",
    "    returns DataFrame with all recipe info\n",
    "    '''\n",
    "    all_recipe_info = pd.DataFrame({\n",
    "        'title': [],\n",
    "        'link': [],\n",
    "        'score': [],\n",
    "        'total_time': [],\n",
    "        'prep': [],\n",
    "        'cook': [],\n",
    "        'num_ing': [],\n",
    "        'num_steps': []\n",
    "    })\n",
    "    for index, row in alldf.iterrows():\n",
    "        if row['rectype'] == 'recipe':\n",
    "            info = tasty_recipe_scraper(row['href'])\n",
    "            all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "        elif row['rectype'] == 'compilation':\n",
    "            info = tasty_compilation_scraper(row['href'])\n",
    "            all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "        info = tasty_compilation_scraper(row['href'])\n",
    "        all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "    return all_recipe_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifteen-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_recipes(recipedf):\n",
    "    '''\n",
    "    scrapes each recipe page in DataFrame recipedf\n",
    "    returns DataFrame with all recipe info\n",
    "    '''\n",
    "    all_recipe_info = pd.DataFrame({\n",
    "        'title': [],\n",
    "        'link': [],\n",
    "        'score': [],\n",
    "        'total_time': [],\n",
    "        'prep': [],\n",
    "        'cook': [],\n",
    "        'num_ing': [],\n",
    "        'num_steps': []\n",
    "    })\n",
    "    for index, row in recipedf.iterrows():\n",
    "        info = tasty_recipe_scraper(row['href'])\n",
    "        all_recipe_info = all_recipe_info.append(info, ignore_index=True)\n",
    "    return all_recipe_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "smaller-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasty_recipes(search, rectype='recipe', sortcol=None):\n",
    "    '''\n",
    "    wrapper function for all tasty scraping\n",
    "    searches tasty.co for all recipes related to search\n",
    "    returns a duplicate-free DataFrame of all rec_type recipes,\n",
    "        sorted on sortcol if provided\n",
    "    rectype: {'recipe', 'compilation', 'all'}\n",
    "    sortcol: {'title', 'link', 'score', 'total_time', 'prep', 'cook', 'num_ing', 'num_steps'}\n",
    "    '''\n",
    "    recipedf = scrape_all_types(tasty_scrape(search, rec_type=rectype))\n",
    "    recipedf.drop_duplicates(subset=\"link\", keep=\"first\", inplace=True)\n",
    "    if sortcol:\n",
    "        recipedf.sort_values(sortcol, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "built-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving webpage...\n",
      "clicking buttons...\n",
      "getting all links...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 452 entries, 0 to 451\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   title       452 non-null    object \n",
      " 1   link        452 non-null    object \n",
      " 2   score       441 non-null    float64\n",
      " 3   total_time  61 non-null     object \n",
      " 4   prep        61 non-null     object \n",
      " 5   cook        61 non-null     object \n",
      " 6   num_ing     452 non-null    float64\n",
      " 7   num_steps   452 non-null    float64\n",
      "dtypes: float64(3), object(5)\n",
      "memory usage: 28.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "all_recipes = scrape_all_types(tasty_scrape('brownie', rec_type='all'))\n",
    "print(all_recipes.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "third-truck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Brownie Truffle-Stuffed Strawberries', 'link': 'https://tasty.co/recipe/brownie-truffle-stuffed-strawberries', 'score': 92, 'total_time': '2 hr 5 min', 'prep': '10 minutes', 'cook': '25 minutes', 'num_ing': 13, 'num_steps': 11}\n"
     ]
    }
   ],
   "source": [
    "info = tasty_recipe_scraper(\"https://tasty.co/recipe/brownie-truffle-stuffed-strawberries\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fourth-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"recipe-time xs-col-12 xs-pr3 md-pr2\"><h5 class=\"extra-bold xs-mb05\">Total<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">2 hr 5 min</p><p class=\"xs-text-4 md-hide\">2 hr 5 min</p></div>, <div class=\"recipe-time xs-col-12 xs-px3 md-px2\"><h5 class=\"extra-bold xs-mb05\">Prep<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">10 minutes</p><p class=\"xs-text-4 md-hide\">10 min</p></div>, <div class=\"recipe-time xs-col-12 xs-pl3 md-pl2\"><h5 class=\"extra-bold xs-mb05\">Cook<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">25 minutes</p><p class=\"xs-text-4 md-hide\">25 min</p></div>, <div class=\"recipe-time xs-col-12 xs-pr3 md-pr2\"><h5 class=\"extra-bold xs-mb05\">Total<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">2 hr 5 min</p><p class=\"xs-text-4 md-hide\">2 hr 5 min</p></div>, <div class=\"recipe-time xs-col-12 xs-px3 md-px2\"><h5 class=\"extra-bold xs-mb05\">Prep<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">10 minutes</p><p class=\"xs-text-4 md-hide\">10 min</p></div>, <div class=\"recipe-time xs-col-12 xs-pl3 md-pl2\"><h5 class=\"extra-bold xs-mb05\">Cook<!-- --> Time</h5><p class=\"xs-text-4 xs-hide md-block\">25 minutes</p><p class=\"xs-text-4 md-hide\">25 min</p></div>]\n"
     ]
    }
   ],
   "source": [
    "times = soup.find_all('div', class_='recipe-time')\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "standing-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "allrecipes = driver.find_elements_by_xpath(\"//a[@class='feed-item']\")\n",
    "print(len(allrecipes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "specified-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://bsmalladi.github.io/\"\n",
    "mypage = requests.get(url)\n",
    "soup = BeautifulSoup(mypage.text, 'html.parser')\n",
    "resume = soup.find('a', class_='nava-resume')\n",
    "resumeurl = url + resume['href'].split('/')[1]\n",
    "myresume = requests.get(resumeurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lasting-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-dayton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
